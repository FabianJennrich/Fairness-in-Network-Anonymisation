{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f233759f-fe98-4b1b-9a19-8ae0444f3b96",
   "metadata": {},
   "source": [
    "## Active Learning Method\n",
    "\n",
    "The idea is to remove regular edges from the graph to make it harder to predict which links will exist. The irregularity of links is determined by the frequency with which they are used in random walks. The idea is that irregular links are more likely to be structurally important.\n",
    "\n",
    "Get a random set of 0.2*n starting noeds, and do a random walk of length m from each one. The links in the walk are tracked, and each time an edge is used, its importance increases. The beta * m most regular links are removed. \n",
    "\n",
    "This algorithm is probabilistic.\n",
    "Time complexity is about O(n* max iter) so O(n^3) when using number of edges, but O(n) when using static maxIter\n",
    "Stated time complexity is O(n*m) because of O(n) number random walks, each with O(m) length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f76e37-bbf7-410a-831c-b60da7ac1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8126cdff-967d-42ed-a9d3-7ac35bcedcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd6221b4-acaa-46d5-94d5-4973c15e25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkImportanceMeasuring(G:nx.Graph, c = 0.1, alpha = 0.2):\n",
    "    '''returns ranked edge list Rlist in ascending order of importance'''\n",
    "    n = G.number_of_nodes()\n",
    "    N = alpha * n\n",
    "    maxIter = 100 ##G.number_of_edges()\n",
    "    \n",
    "    Pt = np.full(n, 1/n) ## P is the distribution to choose from, Pt = P0\n",
    "    ## Pt+1 = (1-c)S^T Pt + c/n * 1 where 1 a suitable 1 vector, S = DA s.t. D=1/di \n",
    "    ## is the probability of ending a RW on a node at time t -> no starting node given\n",
    "    A = nx.adjacency_matrix(G)\n",
    "    D = scipy.sparse.diags_array([1/d for d in dict(G.degree()).values()])\n",
    "    S = D@A\n",
    "    covern_1 = np.full(n, c/n)  \n",
    "\n",
    "    Rt = random.choices(range(n), weights = Pt, k = n)\n",
    "\n",
    "    W = []\n",
    "    for t in range(maxIter):\n",
    "        Ptplus1 = (1-c)* S.T@Pt + covern_1\n",
    "        Rtplus1 = random.choices(range(n), weights = Ptplus1, k = n)\n",
    "        W += [i for i in zip(Rt, Rtplus1)]\n",
    "        Pt = Ptplus1\n",
    "        Rt = Rtplus1\n",
    "    ## based on my understanding of the paper, we sample at each step t from the distribution\n",
    "    ## we do not consider conditional probability\n",
    "\n",
    "    W = [(min(u,v), max(u,v)) for (u,v) in W]\n",
    "    ## update link importance matrix Q based on W, and get list Rlist in order\n",
    "    c = Counter(W)\n",
    "    Rlist = sorted(list(G.edges()), key = lambda e: c[e])\n",
    "    return Rlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb50bbcd-c7c9-423c-9e35-0e04d3fc6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizationLinkPerturbation(G:nx.Graph, beta = 0.1):\n",
    "    '''Takes G and perturbation ratio beta. \n",
    "    returns graph with only the 1-beta most important edges'''\n",
    "    Rlist = linkImportanceMeasuring(G)\n",
    "    Gstar = G.copy()\n",
    "    Gstar.remove_edges_from(Rlist[:int(beta*G.number_of_edges())])\n",
    "    return Gstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ee3434a-9bbb-4df7-96e4-1923ad9db86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAG = nx.barabasi_albert_graph(10000,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d004d3e5-432f-476f-9f7a-1477d87aa111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.162781953811646\n",
      "14.430991888046265\n"
     ]
    }
   ],
   "source": [
    "BAGanon = optimizationLinkPerturbation(BAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "865bd1f7-5428-4550-aca9-6ec8ee9ec35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "150edcb5-19ba-450c-88f4-d646729acbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HBA_10k_82 = nx.read_gml(\"../Data/HBA/HBA_10k_82.gml\", destringizer=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a3b4094-68d0-44d0-b640-339e0c94541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.481199\n",
      "0:00:02.024303\n",
      "0:00:02.023517\n",
      "0:00:02.084436\n",
      "0:00:02.025416\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    start = datetime.datetime.now()\n",
    "    HBA_10k_82_anon = optimizationLinkPerturbation(HBA_10k_82)\n",
    "    nx.write_adjlist(HBA_10k_82_anon, \"../anonymisedGraphs/HBA_10k_82_OLP_29-06_v\"+str(i))\n",
    "    print(datetime.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a8015-25e7-4a4e-b46e-67be6f26c3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
